{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Импорт-библиотек-и-чтение-данных\" data-toc-modified-id=\"Импорт-библиотек-и-чтение-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Импорт библиотек и чтение данных</a></span></li><li><span><a href=\"#Создание-корпуса\" data-toc-modified-id=\"Создание-корпуса-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Создание корпуса</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Мешок-слов\" data-toc-modified-id=\"Мешок-слов-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Мешок слов</a></span><ul class=\"toc-item\"><li><span><a href=\"#Обучение-без-стопслов\" data-toc-modified-id=\"Обучение-без-стопслов-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Обучение без стопслов</a></span></li><li><span><a href=\"#Обучение-со-стопсловами\" data-toc-modified-id=\"Обучение-со-стопсловами-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Обучение со стопсловами</a></span></li></ul></li><li><span><a href=\"#Мешок-слов-с-n-граммами\" data-toc-modified-id=\"Мешок-слов-с-n-граммами-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Мешок слов с n-граммами</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>BERT</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» (c BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек и чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import notebook \n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим дополнительные компоненты библиотеки `nltk`, такие как **stopwords** и **punkt**, необходимые для дальнешей работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\femt0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\femt0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') \n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем данные из файла в переменную типа DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./datasets/toxic_comments.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим баланс классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отношение отрицательного класса к положительному составляет 8.8:1\n"
     ]
    }
   ],
   "source": [
    "balance = data['toxic'].value_counts()[0] / data['toxic'].value_counts()[1]\n",
    "print(\n",
    "    f'Отношение отрицательного класса к положительному составляет {balance:.1f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143346"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:**\n",
    "\n",
    "Данные не требуют дополнительной подготовки к дальнешей работе - пропуски отсутствуют, типы данных соответствуют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание корпуса\n",
    "\n",
    "Приведём текст сообщений единообразной форме, т.е. сформируем корпус.\n",
    "\n",
    "в том числе приведём слова к их лемме, и очистим от \"мусорных\" слов и символов.\n",
    "\n",
    "1. Токенизируем текст с используем библиотеки `nltk.tokenize.TweetTokenizer`, как более подходящее под тип текста (интернет-общение):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "tokenized = data['text'].str.lower().apply(tknzr.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Очистим текст, оставив только буквы латинского алфавита с использованием регулярных выражений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    clean = re.sub(r'[^a-zA-Z\\' ]', ' ', text)\n",
    "    return \" \".join(clean.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20ffce8e6944eb7847cd50c1bd59adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_corpus = []\n",
    "for text in notebook.tqdm(tokenized.values):\n",
    "    clean_corpus.append(clean(\" \".join(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Воспользуемся лемматизатором `WordNetLemmatizer` из той же библиотеки `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae6c5a32c424992870098d5e1434504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatized = []\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for text in notebook.tqdm(clean_corpus):\n",
    "    lemmatized.append(\" \".join([wnl.lemmatize(word) for word in text.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Добавим полученный результат обратно в таблицу в виде нового столбца:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmatized_text'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww he match this background colour i'm seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i'm really not trying to edit war it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can't make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  explanation why the edits made under my userna...  \n",
       "1  d'aww he match this background colour i'm seem...  \n",
       "2  hey man i'm really not trying to edit war it's...  \n",
       "3  more i can't make any real suggestion on impro...  \n",
       "4  you sir are my hero any chance you remember wh...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим признаки в переменные **X** и **y**, и разделим данные на тренировочную и валидационную выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['lemmatized_text'].copy()\n",
    "y = data['toxic'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В связи с дисбалансом классов опробуем вариант выравнивания этого баланса методом \"**upsample**\" (увеличение выборки). На данных выборках будем обучать только модели, менее склонные к переобучению, т.е. линейные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(X, y, up_coeff):\n",
    "    X_zeros = X[y == 0]\n",
    "    X_ones = X[y == 1]\n",
    "    y_zeros = y[y == 0]\n",
    "    y_ones = y[y == 1]\n",
    "    \n",
    "    X_upsampled, y_upsampled = shuffle(\n",
    "        pd.concat([X_zeros] + [X_ones] * up_coeff),\n",
    "        pd.concat([y_zeros] + [y_ones] * up_coeff),\n",
    "        random_state=13)\n",
    "    \n",
    "    return X_upsampled, y_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим переменные, использующие данный метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Классы       0          1      \n",
      " imbalanced   114593     13063    \n",
      "  balanced    114593     104504   \n"
     ]
    }
   ],
   "source": [
    "X_train_up, y_train_up = upsample(X_train, y_train, 8)\n",
    "\n",
    "print(f'{\"Классы\": ^12}{\"0\": ^10}{\"1\": ^12}')\n",
    "print(f'{\"imbalanced\": ^12}{y_train.value_counts()[0]: ^10}{y_train.value_counts()[1]: ^12}')\n",
    "print(f'{\"balanced\": ^12}{y_train_up.value_counts()[0]: ^10}{y_train_up.value_counts()[1]: ^12}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Текстовые данные приведены к единообразной форме, и готовы к обучению моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты работы всех моделей будут собраны в новый *DataFrame* **results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['method', 'model_name', 'F1', 'fit_time', 'predict_time']\n",
    "\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение будем производить на одинаковых моделях, в независимости от способа подготовки данных, а именно `LogisticRegression()`, `Perceptron()`,`DecisionTreeClassifier()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(max_iter=1500),\n",
    "    Perceptron(),\n",
    "    DecisionTreeClassifier(max_depth=100, random_state=13)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим функцию, заполняющая таблицу с результатами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(model, method, result_df=results,\n",
    "           X=X_train,\n",
    "           y=y_train,\n",
    "           X_tru=X_test,\n",
    "           y_tru=y_test):\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    time_start = datetime.now()\n",
    "    model.fit(X, y)\n",
    "    time_mid = datetime.now()\n",
    "    y_pred = model.predict(X_tru)\n",
    "    f1_ = f1_score(y_tru, y_pred)\n",
    "    time_end = datetime.now()\n",
    "    result_df.loc[result_df.shape[0]] = [method, model_name, f1_,\n",
    "                               (time_mid - time_start).total_seconds(),\n",
    "                               (time_end - time_mid).total_seconds()]\n",
    "    print(f'Model {model_name}, F1 = {f1_}')\n",
    "    #return model, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в таблицу значение **baseline**, для оценки адекватности моделей. В качестве таковой используем `DummyClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model DummyClassifier, F1 = 0.18028907831342475\n"
     ]
    }
   ],
   "source": [
    "baseline = DummyClassifier(strategy=\"constant\", random_state=13, constant=1)\n",
    "\n",
    "record(model=baseline,\n",
    "       method='baseline',\n",
    "       X=X_train, y=y_train,\n",
    "       X_tru=X_test, y_tru=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем текст в векторный формат методом **\"мешок слов\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение без стопслов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим \"мешок слов\" с удалением \"**стопслов**\", загруженных раннее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер мешка слов с учётом стоп-слов:\n",
      "train: 127656 на 139030\n",
      "test: 31915 на 139030\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(stop_words=stopwords) \n",
    "bow_train = count_vect.fit_transform(X_train.values)\n",
    "bow_test = count_vect.transform(X_test.values)\n",
    "\n",
    "print('Размер мешка слов с учётом стоп-слов:')\n",
    "print(f'train: {bow_train.shape[0]} на {bow_train.shape[1]}')\n",
    "print(f'test: {bow_test.shape[0]} на {bow_test.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведём обучение с использованием вышеуказанных моделей (`LogisticRegression()`, `Perceptron()`,`DecisionTreeClassifier()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03082c7b2c04321a14a7b5fc792700a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression, F1 = 0.7526278906797479\n",
      "Model Perceptron, F1 = 0.7004073958006894\n",
      "Model DecisionTreeClassifier, F1 = 0.7101300479123887\n"
     ]
    }
   ],
   "source": [
    "for model in notebook.tqdm(models):\n",
    "    record(model=model,\n",
    "           method='BOW w/ stopwords',\n",
    "           X=bow_train,\n",
    "           X_tru=bow_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение со стопсловами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также проверим аналогичный подход, не удаляя \"*стопслова*\", как предположительно необходимые для определения \"*токсичности*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер мешка слов без учёта стоп-слов:\n",
      "train: 127656 на 139174\n",
      "test: 31915 на 139174\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer() \n",
    "bow_train_2 = count_vect.fit_transform(X_train.values)\n",
    "bow_test_2 = count_vect.transform(X_test.values)\n",
    "\n",
    "print('Размер мешка слов без учёта стоп-слов:')\n",
    "print(f'train: {bow_train_2.shape[0]} на {bow_train_2.shape[1]}')\n",
    "print(f'test: {bow_test_2.shape[0]} на {bow_test_2.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичным образом проверим результат для обучения в полном объёме данных, с сохранёнными стопсловами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9907cc7d2ab45bdba3bf4d97b4cedda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression, F1 = 0.771295818275684\n",
      "Model Perceptron, F1 = 0.7283086439850237\n",
      "Model DecisionTreeClassifier, F1 = 0.715045462343455\n"
     ]
    }
   ],
   "source": [
    "for model in notebook.tqdm(models):\n",
    "    record(model=model,\n",
    "           method='BOW w/o stopwords',\n",
    "           X=bow_train_2,\n",
    "           X_tru=bow_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мешок слов с n-граммами\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим мешок слов образованием словосочетаний (**N-грамм**), по основанию 1 и 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер мешка слов c N-граммами:\n",
      "train: 127656 на 1892362\n",
      "test: 31915 на 1892362\n"
     ]
    }
   ],
   "source": [
    "n_gramm_count_vect = CountVectorizer(ngram_range=(1,2)) \n",
    "n_gramm_train = n_gramm_count_vect.fit_transform(X_train.values)\n",
    "n_gramm_test = n_gramm_count_vect.transform(X_test.values)\n",
    "\n",
    "\n",
    "print('Размер мешка слов c N-граммами:')\n",
    "print(f'train: {n_gramm_train.shape[0]} на {n_gramm_train.shape[1]}')\n",
    "print(f'test: {n_gramm_test.shape[0]} на {n_gramm_test.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим результаты обучения на основе мешка слов с n-граммами (в данном случае - униграммы и биграммы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1407c59594b480696c1e5cc8d04c995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression, F1 = 0.7753534648280677\n",
      "Model Perceptron, F1 = 0.7640521823159929\n",
      "Model DecisionTreeClassifier, F1 = 0.7144089732528042\n"
     ]
    }
   ],
   "source": [
    "for model in notebook.tqdm(models):\n",
    "    record(model=model,\n",
    "           method='N_gram',\n",
    "           X=n_gramm_train,\n",
    "           X_tru=n_gramm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также проверим мешок слов с n-граммами на отбалансированных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер мешка слов c N-граммами:\n",
      "train: 219097 на 1892362\n",
      "test: 31915 на 1892362\n"
     ]
    }
   ],
   "source": [
    "n_gramm_train_up = n_gramm_count_vect.fit_transform(X_train_up.values)\n",
    "n_gramm_test_up = n_gramm_count_vect.transform(X_test.values)\n",
    "\n",
    "\n",
    "print('Размер мешка слов c N-граммами:')\n",
    "print(f'train: {n_gramm_train_up.shape[0]} на {n_gramm_train_up.shape[1]}')\n",
    "print(f'test: {n_gramm_test_up.shape[0]} на {n_gramm_test_up.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e0075adb07422aa58f67dea5e501ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression, F1 = 0.7957263594323075\n",
      "Model Perceptron, F1 = 0.7707850281550184\n"
     ]
    }
   ],
   "source": [
    "for model in notebook.tqdm([LogisticRegression(max_iter=1500), Perceptron()]):\n",
    "    record(model=model,\n",
    "           method='N_gram_upsampled',\n",
    "           X=n_gramm_train_up, y=y_train_up,\n",
    "           X_tru=n_gramm_test_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим матрицы частоты употребления слов **TF-IDF** с использованием части библиотеки `sklearn` под названием `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матриц TF-IDF:\n",
      "train: 127656 на 139030\n",
      "test: 31915 на 139030\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords) \n",
    "X_train_tf = count_tf_idf.fit_transform(X_train.values.astype('U'))\n",
    "X_test_tf = count_tf_idf.transform(X_test.values.astype('U'))\n",
    "\n",
    "\n",
    "print('Размер матриц TF-IDF:')\n",
    "print(f'train: {X_train_tf.shape[0]} на {X_train_tf.shape[1]}')\n",
    "print(f'test: {X_test_tf.shape[0]} на {X_test_tf.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичным образом добавим результаты для моделей с **TF-IDF**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a589229ed2db47cf87258206430158bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression, F1 = 0.7270296084049666\n",
      "Model Perceptron, F1 = 0.7098674521354933\n",
      "Model DecisionTreeClassifier, F1 = 0.7237206085753803\n"
     ]
    }
   ],
   "source": [
    "for model in notebook.tqdm(models):\n",
    "    record(model=model,\n",
    "           method='TF_IDF',\n",
    "           X=X_train_tf,\n",
    "           X_tru=X_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также проверим данный метод на отбалансированных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матриц TF-IDF:\n",
      "train: 219097 на 139030\n",
      "test: 31915 на 139030\n"
     ]
    }
   ],
   "source": [
    "X_train_tf_up = count_tf_idf.fit_transform(X_train_up.values.astype('U'))\n",
    "X_test_tf_up = count_tf_idf.transform(X_test.values.astype('U'))\n",
    "\n",
    "print('Размер матриц TF-IDF:')\n",
    "print(f'train: {X_train_tf_up.shape[0]} на {X_train_tf_up.shape[1]}')\n",
    "print(f'test: {X_test_tf_up.shape[0]} на {X_test_tf_up.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcaa487cda2499ca6b71757cfa4f913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression, F1 = 0.7504584567640006\n",
      "Model Perceptron, F1 = 0.7001044932079414\n"
     ]
    }
   ],
   "source": [
    "for model in notebook.tqdm([LogisticRegression(max_iter=1500), Perceptron()]):\n",
    "    record(model=model,\n",
    "           method='TF_IDF_upsampled',\n",
    "           X=X_train_tf_up, y=y_train_up,\n",
    "           X_tru=X_test_tf_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся библиотекой **BERT** (*Bidirectional Encoder Representations from Transformers*) за авторством Google, для создания векторного представления корпуса текстов.\n",
    "\n",
    "Воспользуемся предобученной моделью `bert-base-uncased` для наших целей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer(vocab_file='./datasets/vocab.txt')\n",
    "\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим устройство на наличие CUDA-совместимых ускорителей, и перенесём туда нашу модель BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем текст методами BERT (преобразуем текст в номера токенов словаря). Максимальную длину устанавливаем равную 512, при больших значениях предобученная модель останавливается с ошибкой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = data['lemmatized_text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True,\n",
    "                                                                      max_length=512,\n",
    "                                                                      truncation=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выравниваем длины исходных текстов в корпусе, и создаем маску:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 512)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = tokenized.str.len().max()\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём векторное представление (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba7ba4a88214924ab427b5d8d6d09ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1063 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 150\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch.to(device), attention_mask=attention_mask_batch.to(device))\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "\n",
    "        \n",
    "        if (i == padded.shape[0] // batch_size - 1) and (padded.shape[0] % batch_size != 0):\n",
    "            #last piece of \"cake\"\n",
    "            last_piece = padded.shape[0] % batch_size\n",
    "            batch = torch.LongTensor(\n",
    "                padded[batch_size*(i+1):(batch_size*(i+1)+last_piece)]\n",
    "            )\n",
    "            attention_mask_batch = torch.LongTensor(\n",
    "                attention_mask[batch_size*(i+1):(batch_size*(i+1)+last_piece)]\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = model(batch.to(device), attention_mask=attention_mask_batch.to(device))\n",
    "            embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "            \n",
    "            del batch\n",
    "            del attention_mask_batch\n",
    "            del batch_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединяем результат в один список, и разделяем его в тренировочную и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матриц эмбеддингов:\n",
      "train: 119678 на 768\n",
      "test: 39893 на 768\n"
     ]
    }
   ],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data['toxic'], test_size=0.25, random_state=13)\n",
    "X_train_up, y_train_up = upsample(pd.DataFrame(X_train, index=y_train.index), y_train, 8)\n",
    "\n",
    "print('Размер матриц эмбеддингов:')\n",
    "print(f'train: {X_train.shape[0]} на {X_train.shape[1]}')\n",
    "print(f'test: {X_test.shape[0]} на {X_test.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также обучаем модели и получаем результаты метрики **F1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951b906cf16c4470959cd7766260a9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression, F1 = 0.7129381951288187\n",
      "Model Perceptron, F1 = 0.5544539116963594\n"
     ]
    }
   ],
   "source": [
    "for model in notebook.tqdm([LogisticRegression(max_iter=1500), Perceptron()]):\n",
    "    record(model=model,\n",
    "           method='BERT',\n",
    "           X=X_train, y=y_train,\n",
    "           X_tru=X_test, y_tru=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8567b66c19646418fdeb92b9a845e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression, F1 = 0.6384159881569209\n",
      "Model Perceptron, F1 = 0.5983052298211076\n"
     ]
    }
   ],
   "source": [
    "for model in notebook.tqdm([LogisticRegression(max_iter=1500), Perceptron()]):\n",
    "    record(model=model,\n",
    "           method='BERT_upsampled',\n",
    "           X=X_train_up, y=y_train_up,\n",
    "           X_tru=X_test, y_tru=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Все модели обучены, результаты собраны в одну таблицу, готовую к финальному анализу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим полученную таблицу с результатами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3cfe6_row0_col0, #T_3cfe6_row0_col1, #T_3cfe6_row0_col2, #T_3cfe6_row2_col1, #T_3cfe6_row5_col1, #T_3cfe6_row5_col2, #T_3cfe6_row13_col1, #T_3cfe6_row16_col1 {\n",
       "  background-color: #f7fcf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row1_col0 {\n",
       "  background-color: #085598;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row1_col1 {\n",
       "  background-color: #ecf8e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row1_col2, #T_3cfe6_row3_col2, #T_3cfe6_row10_col2, #T_3cfe6_row12_col2, #T_3cfe6_row13_col2, #T_3cfe6_row15_col2, #T_3cfe6_row16_col2 {\n",
       "  background-color: #daf0d4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row2_col0, #T_3cfe6_row16_col0 {\n",
       "  background-color: #1070b0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row2_col2 {\n",
       "  background-color: #e6f5e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row3_col0, #T_3cfe6_row13_col0 {\n",
       "  background-color: #0b6cae;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row3_col1 {\n",
       "  background-color: #e2f4dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row4_col0, #T_3cfe6_row11_col0 {\n",
       "  background-color: #084d8e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row4_col1 {\n",
       "  background-color: #e5f5e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row4_col2 {\n",
       "  background-color: #e9f7e3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row5_col0, #T_3cfe6_row12_col0 {\n",
       "  background-color: #0863a7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row6_col0, #T_3cfe6_row9_col0 {\n",
       "  background-color: #0969ad;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row6_col1 {\n",
       "  background-color: #daf1d5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row6_col2, #T_3cfe6_row9_col2, #T_3cfe6_row14_col2 {\n",
       "  background-color: #b5e2bb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row7_col0 {\n",
       "  background-color: #084a8c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row7_col1 {\n",
       "  background-color: #41a5cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row7_col2 {\n",
       "  background-color: #cdebc6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row8_col0 {\n",
       "  background-color: #085093;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row8_col1, #T_3cfe6_row11_col1, #T_3cfe6_row18_col1, #T_3cfe6_row20_col1 {\n",
       "  background-color: #f6fcef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row8_col2, #T_3cfe6_row11_col2 {\n",
       "  background-color: #d9f0d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row9_col1 {\n",
       "  background-color: #084889;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row10_col0, #T_3cfe6_row10_col1, #T_3cfe6_row19_col2 {\n",
       "  background-color: #084081;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row12_col1 {\n",
       "  background-color: #f5fbee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row14_col0 {\n",
       "  background-color: #0864a8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row14_col1 {\n",
       "  background-color: #d8f0d2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row15_col0 {\n",
       "  background-color: #085799;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row15_col1 {\n",
       "  background-color: #f3faec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row17_col0 {\n",
       "  background-color: #0a6aad;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row17_col1 {\n",
       "  background-color: #ddf2d8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row17_col2 {\n",
       "  background-color: #1373b2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row18_col0 {\n",
       "  background-color: #54b6d1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row18_col2 {\n",
       "  background-color: #1171b1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row19_col0 {\n",
       "  background-color: #2c8ebf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row19_col1 {\n",
       "  background-color: #d6efd0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3cfe6_row20_col0 {\n",
       "  background-color: #3fa2ca;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3cfe6_row20_col2 {\n",
       "  background-color: #0862a5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3cfe6_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >F1</th>\n",
       "      <th class=\"col_heading level0 col1\" >fit_time</th>\n",
       "      <th class=\"col_heading level0 col2\" >predict_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >method</th>\n",
       "      <th class=\"index_name level1\" >model_name</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row0\" class=\"row_heading level0 row0\" >baseline</th>\n",
       "      <th id=\"T_3cfe6_level1_row0\" class=\"row_heading level1 row0\" >DummyClassifier</th>\n",
       "      <td id=\"T_3cfe6_row0_col0\" class=\"data row0 col0\" >0.180289</td>\n",
       "      <td id=\"T_3cfe6_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "      <td id=\"T_3cfe6_row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row1\" class=\"row_heading level0 row1\" rowspan=\"3\">BOW w/ stopwords</th>\n",
       "      <th id=\"T_3cfe6_level1_row1\" class=\"row_heading level1 row1\" >LogisticRegression</th>\n",
       "      <td id=\"T_3cfe6_row1_col0\" class=\"data row1 col0\" >0.752628</td>\n",
       "      <td id=\"T_3cfe6_row1_col1\" class=\"data row1 col1\" >16.151746</td>\n",
       "      <td id=\"T_3cfe6_row1_col2\" class=\"data row1 col2\" >0.015629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row2\" class=\"row_heading level1 row2\" >Perceptron</th>\n",
       "      <td id=\"T_3cfe6_row2_col0\" class=\"data row2 col0\" >0.700407</td>\n",
       "      <td id=\"T_3cfe6_row2_col1\" class=\"data row2 col1\" >0.618104</td>\n",
       "      <td id=\"T_3cfe6_row2_col2\" class=\"data row2 col2\" >0.009009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row3\" class=\"row_heading level1 row3\" >DecisionTreeClassifier</th>\n",
       "      <td id=\"T_3cfe6_row3_col0\" class=\"data row3 col0\" >0.710130</td>\n",
       "      <td id=\"T_3cfe6_row3_col1\" class=\"data row3 col1\" >29.652154</td>\n",
       "      <td id=\"T_3cfe6_row3_col2\" class=\"data row3 col2\" >0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"3\">BOW w/o stopwords</th>\n",
       "      <th id=\"T_3cfe6_level1_row4\" class=\"row_heading level1 row4\" >LogisticRegression</th>\n",
       "      <td id=\"T_3cfe6_row4_col0\" class=\"data row4 col0\" >0.771296</td>\n",
       "      <td id=\"T_3cfe6_row4_col1\" class=\"data row4 col1\" >25.979635</td>\n",
       "      <td id=\"T_3cfe6_row4_col2\" class=\"data row4 col2\" >0.007005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row5\" class=\"row_heading level1 row5\" >Perceptron</th>\n",
       "      <td id=\"T_3cfe6_row5_col0\" class=\"data row5 col0\" >0.728309</td>\n",
       "      <td id=\"T_3cfe6_row5_col1\" class=\"data row5 col1\" >0.454837</td>\n",
       "      <td id=\"T_3cfe6_row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row6\" class=\"row_heading level1 row6\" >DecisionTreeClassifier</th>\n",
       "      <td id=\"T_3cfe6_row6_col0\" class=\"data row6 col0\" >0.715045</td>\n",
       "      <td id=\"T_3cfe6_row6_col1\" class=\"data row6 col1\" >41.870684</td>\n",
       "      <td id=\"T_3cfe6_row6_col2\" class=\"data row6 col2\" >0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row7\" class=\"row_heading level0 row7\" rowspan=\"3\">N_gram</th>\n",
       "      <th id=\"T_3cfe6_level1_row7\" class=\"row_heading level1 row7\" >LogisticRegression</th>\n",
       "      <td id=\"T_3cfe6_row7_col0\" class=\"data row7 col0\" >0.775353</td>\n",
       "      <td id=\"T_3cfe6_row7_col1\" class=\"data row7 col1\" >173.404314</td>\n",
       "      <td id=\"T_3cfe6_row7_col2\" class=\"data row7 col2\" >0.023020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row8\" class=\"row_heading level1 row8\" >Perceptron</th>\n",
       "      <td id=\"T_3cfe6_row8_col0\" class=\"data row8 col0\" >0.764052</td>\n",
       "      <td id=\"T_3cfe6_row8_col1\" class=\"data row8 col1\" >1.261852</td>\n",
       "      <td id=\"T_3cfe6_row8_col2\" class=\"data row8 col2\" >0.016026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row9\" class=\"row_heading level1 row9\" >DecisionTreeClassifier</th>\n",
       "      <td id=\"T_3cfe6_row9_col0\" class=\"data row9 col0\" >0.714409</td>\n",
       "      <td id=\"T_3cfe6_row9_col1\" class=\"data row9 col1\" >252.081907</td>\n",
       "      <td id=\"T_3cfe6_row9_col2\" class=\"data row9 col2\" >0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row10\" class=\"row_heading level0 row10\" rowspan=\"2\">N_gram_upsampled</th>\n",
       "      <th id=\"T_3cfe6_level1_row10\" class=\"row_heading level1 row10\" >LogisticRegression</th>\n",
       "      <td id=\"T_3cfe6_row10_col0\" class=\"data row10 col0\" >0.795726</td>\n",
       "      <td id=\"T_3cfe6_row10_col1\" class=\"data row10 col1\" >258.851020</td>\n",
       "      <td id=\"T_3cfe6_row10_col2\" class=\"data row10 col2\" >0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row11\" class=\"row_heading level1 row11\" >Perceptron</th>\n",
       "      <td id=\"T_3cfe6_row11_col0\" class=\"data row11 col0\" >0.770785</td>\n",
       "      <td id=\"T_3cfe6_row11_col1\" class=\"data row11 col1\" >1.929641</td>\n",
       "      <td id=\"T_3cfe6_row11_col2\" class=\"data row11 col2\" >0.016026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row12\" class=\"row_heading level0 row12\" rowspan=\"3\">TF_IDF</th>\n",
       "      <th id=\"T_3cfe6_level1_row12\" class=\"row_heading level1 row12\" >LogisticRegression</th>\n",
       "      <td id=\"T_3cfe6_row12_col0\" class=\"data row12 col0\" >0.727030</td>\n",
       "      <td id=\"T_3cfe6_row12_col1\" class=\"data row12 col1\" >3.299408</td>\n",
       "      <td id=\"T_3cfe6_row12_col2\" class=\"data row12 col2\" >0.015624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row13\" class=\"row_heading level1 row13\" >Perceptron</th>\n",
       "      <td id=\"T_3cfe6_row13_col0\" class=\"data row13 col0\" >0.709867</td>\n",
       "      <td id=\"T_3cfe6_row13_col1\" class=\"data row13 col1\" >0.251157</td>\n",
       "      <td id=\"T_3cfe6_row13_col2\" class=\"data row13 col2\" >0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row14\" class=\"row_heading level1 row14\" >DecisionTreeClassifier</th>\n",
       "      <td id=\"T_3cfe6_row14_col0\" class=\"data row14 col0\" >0.723721</td>\n",
       "      <td id=\"T_3cfe6_row14_col1\" class=\"data row14 col1\" >46.322238</td>\n",
       "      <td id=\"T_3cfe6_row14_col2\" class=\"data row14 col2\" >0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row15\" class=\"row_heading level0 row15\" rowspan=\"2\">TF_IDF_upsampled</th>\n",
       "      <th id=\"T_3cfe6_level1_row15\" class=\"row_heading level1 row15\" >LogisticRegression</th>\n",
       "      <td id=\"T_3cfe6_row15_col0\" class=\"data row15 col0\" >0.750458</td>\n",
       "      <td id=\"T_3cfe6_row15_col1\" class=\"data row15 col1\" >6.327015</td>\n",
       "      <td id=\"T_3cfe6_row15_col2\" class=\"data row15 col2\" >0.015623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row16\" class=\"row_heading level1 row16\" >Perceptron</th>\n",
       "      <td id=\"T_3cfe6_row16_col0\" class=\"data row16 col0\" >0.700104</td>\n",
       "      <td id=\"T_3cfe6_row16_col1\" class=\"data row16 col1\" >0.408090</td>\n",
       "      <td id=\"T_3cfe6_row16_col2\" class=\"data row16 col2\" >0.015634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row17\" class=\"row_heading level0 row17\" rowspan=\"2\">BERT</th>\n",
       "      <th id=\"T_3cfe6_level1_row17\" class=\"row_heading level1 row17\" >LogisticRegression</th>\n",
       "      <td id=\"T_3cfe6_row17_col0\" class=\"data row17 col0\" >0.712938</td>\n",
       "      <td id=\"T_3cfe6_row17_col1\" class=\"data row17 col1\" >37.322551</td>\n",
       "      <td id=\"T_3cfe6_row17_col2\" class=\"data row17 col2\" >0.078509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row18\" class=\"row_heading level1 row18\" >Perceptron</th>\n",
       "      <td id=\"T_3cfe6_row18_col0\" class=\"data row18 col0\" >0.554454</td>\n",
       "      <td id=\"T_3cfe6_row18_col1\" class=\"data row18 col1\" >1.553145</td>\n",
       "      <td id=\"T_3cfe6_row18_col2\" class=\"data row18 col2\" >0.079406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level0_row19\" class=\"row_heading level0 row19\" rowspan=\"2\">BERT_upsampled</th>\n",
       "      <th id=\"T_3cfe6_level1_row19\" class=\"row_heading level1 row19\" >LogisticRegression</th>\n",
       "      <td id=\"T_3cfe6_row19_col0\" class=\"data row19 col0\" >0.638416</td>\n",
       "      <td id=\"T_3cfe6_row19_col1\" class=\"data row19 col1\" >48.935536</td>\n",
       "      <td id=\"T_3cfe6_row19_col2\" class=\"data row19 col2\" >0.094133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cfe6_level1_row20\" class=\"row_heading level1 row20\" >Perceptron</th>\n",
       "      <td id=\"T_3cfe6_row20_col0\" class=\"data row20 col0\" >0.598305</td>\n",
       "      <td id=\"T_3cfe6_row20_col1\" class=\"data row20 col1\" >1.988012</td>\n",
       "      <td id=\"T_3cfe6_row20_col2\" class=\"data row20 col2\" >0.084067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c08b7e8e20>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.set_index(['method', 'model_name']).style.background_gradient(cmap='GnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Все модели проходят проверку на адекватность: метрика F1 во много раз больше baseline метрики равной **0.18**;\n",
    "2. Удаление стопслов из корпуса сказалось на результате в сторону ухудшения, предположительно, в случае оценки токсичности - они имеют большее значение, чем ожидалось;\n",
    "3. Наглядно видно, что **BERT** в данном случае показывает наихудщие результаты, не считая времени необходимого на перевод текста в векторные представления (*embeddings*). Основной причиной, по видимому, является использование предобученной модели `bert-base-uncased`, которая не учитывает специфику наших исходных данных. Также модели BERT балансировка класса методом увеличения выборки (upsample) не приносит должного результата\n",
    "\n",
    "Выберем из таблицы результатов только удовлетворяющие поставленному условию - метрика ${F1\\geq0.75}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5957e_row0_col2 {\n",
       "  background-color: #eef9e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row0_col3 {\n",
       "  background-color: #edf8e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row0_col4, #T_5957e_row4_col4, #T_5957e_row6_col4 {\n",
       "  background-color: #6ec5c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row1_col2 {\n",
       "  background-color: #8ad2bf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row1_col3 {\n",
       "  background-color: #e6f5e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row1_col4, #T_5957e_row3_col3, #T_5957e_row5_col3, #T_5957e_row6_col2 {\n",
       "  background-color: #f7fcf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row2_col2 {\n",
       "  background-color: #69c2ca;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row2_col3 {\n",
       "  background-color: #41a5cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5957e_row2_col4, #T_5957e_row4_col2, #T_5957e_row4_col3 {\n",
       "  background-color: #084081;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5957e_row3_col2 {\n",
       "  background-color: #bee6bf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row3_col4, #T_5957e_row5_col4 {\n",
       "  background-color: #64bfcc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row5_col2 {\n",
       "  background-color: #8ed3be;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5957e_row6_col3 {\n",
       "  background-color: #f3fbed;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5957e_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >method</th>\n",
       "      <th class=\"col_heading level0 col1\" >model_name</th>\n",
       "      <th class=\"col_heading level0 col2\" >F1</th>\n",
       "      <th class=\"col_heading level0 col3\" >fit_time</th>\n",
       "      <th class=\"col_heading level0 col4\" >predict_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5957e_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_5957e_row0_col0\" class=\"data row0 col0\" >BOW w/ stopwords</td>\n",
       "      <td id=\"T_5957e_row0_col1\" class=\"data row0 col1\" >LogisticRegression</td>\n",
       "      <td id=\"T_5957e_row0_col2\" class=\"data row0 col2\" >0.752628</td>\n",
       "      <td id=\"T_5957e_row0_col3\" class=\"data row0 col3\" >16.151746</td>\n",
       "      <td id=\"T_5957e_row0_col4\" class=\"data row0 col4\" >0.015629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5957e_level0_row1\" class=\"row_heading level0 row1\" >4</th>\n",
       "      <td id=\"T_5957e_row1_col0\" class=\"data row1 col0\" >BOW w/o stopwords</td>\n",
       "      <td id=\"T_5957e_row1_col1\" class=\"data row1 col1\" >LogisticRegression</td>\n",
       "      <td id=\"T_5957e_row1_col2\" class=\"data row1 col2\" >0.771296</td>\n",
       "      <td id=\"T_5957e_row1_col3\" class=\"data row1 col3\" >25.979635</td>\n",
       "      <td id=\"T_5957e_row1_col4\" class=\"data row1 col4\" >0.007005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5957e_level0_row2\" class=\"row_heading level0 row2\" >7</th>\n",
       "      <td id=\"T_5957e_row2_col0\" class=\"data row2 col0\" >N_gram</td>\n",
       "      <td id=\"T_5957e_row2_col1\" class=\"data row2 col1\" >LogisticRegression</td>\n",
       "      <td id=\"T_5957e_row2_col2\" class=\"data row2 col2\" >0.775353</td>\n",
       "      <td id=\"T_5957e_row2_col3\" class=\"data row2 col3\" >173.404314</td>\n",
       "      <td id=\"T_5957e_row2_col4\" class=\"data row2 col4\" >0.023020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5957e_level0_row3\" class=\"row_heading level0 row3\" >8</th>\n",
       "      <td id=\"T_5957e_row3_col0\" class=\"data row3 col0\" >N_gram</td>\n",
       "      <td id=\"T_5957e_row3_col1\" class=\"data row3 col1\" >Perceptron</td>\n",
       "      <td id=\"T_5957e_row3_col2\" class=\"data row3 col2\" >0.764052</td>\n",
       "      <td id=\"T_5957e_row3_col3\" class=\"data row3 col3\" >1.261852</td>\n",
       "      <td id=\"T_5957e_row3_col4\" class=\"data row3 col4\" >0.016026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5957e_level0_row4\" class=\"row_heading level0 row4\" >10</th>\n",
       "      <td id=\"T_5957e_row4_col0\" class=\"data row4 col0\" >N_gram_upsampled</td>\n",
       "      <td id=\"T_5957e_row4_col1\" class=\"data row4 col1\" >LogisticRegression</td>\n",
       "      <td id=\"T_5957e_row4_col2\" class=\"data row4 col2\" >0.795726</td>\n",
       "      <td id=\"T_5957e_row4_col3\" class=\"data row4 col3\" >258.851020</td>\n",
       "      <td id=\"T_5957e_row4_col4\" class=\"data row4 col4\" >0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5957e_level0_row5\" class=\"row_heading level0 row5\" >11</th>\n",
       "      <td id=\"T_5957e_row5_col0\" class=\"data row5 col0\" >N_gram_upsampled</td>\n",
       "      <td id=\"T_5957e_row5_col1\" class=\"data row5 col1\" >Perceptron</td>\n",
       "      <td id=\"T_5957e_row5_col2\" class=\"data row5 col2\" >0.770785</td>\n",
       "      <td id=\"T_5957e_row5_col3\" class=\"data row5 col3\" >1.929641</td>\n",
       "      <td id=\"T_5957e_row5_col4\" class=\"data row5 col4\" >0.016026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5957e_level0_row6\" class=\"row_heading level0 row6\" >15</th>\n",
       "      <td id=\"T_5957e_row6_col0\" class=\"data row6 col0\" >TF_IDF_upsampled</td>\n",
       "      <td id=\"T_5957e_row6_col1\" class=\"data row6 col1\" >LogisticRegression</td>\n",
       "      <td id=\"T_5957e_row6_col2\" class=\"data row6 col2\" >0.750458</td>\n",
       "      <td id=\"T_5957e_row6_col3\" class=\"data row6 col3\" >6.327015</td>\n",
       "      <td id=\"T_5957e_row6_col4\" class=\"data row6 col4\" >0.015623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c0a36e8760>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.query('F1 > 0.75').style.background_gradient(cmap='GnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Финальный вывод**\n",
    "\n",
    "1. Лучшие результы как метрики, так и времени обучения показывает модель логической регрессии;\n",
    "2. **Максимальный** достижимый результат метрики F1 равный **0.796** был достигнут при использовании *LogisticRegression* с использованием мешка слова с N-граммами (в данном случае уни- и биграммы) на сбалансированный методом upsampling выборке;\n",
    "3. Самый быстрый, из проходящих по условию ${F1\\geq0.75}$ - самая старая из моделей, перцептрон, с метрикой F1 равной **0.77** (с применением n-грамм на наборе данных с upsampling), и временем обучения в 1.9 секунды.\n",
    "4. При балансировании классов, метод TF-IDF также начинает показывать результат равный требуемому **0.75**, при этом обладая хорошим временм работы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
